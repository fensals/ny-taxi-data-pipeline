{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d96847d6-0e98-4363-8837-9b9e2dd5bdc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c6d96b9-5dcc-40c7-9f5e-f38e9ae012a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Femi\n"
     ]
    }
   ],
   "source": [
    "print('Femi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aed7d72-642d-4ab8-ae85-e4efa8c06808",
   "metadata": {},
   "source": [
    "#####  Pyarrow.parquet reads a parquet file as a table  which can be converted to a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e06de404-8db7-4c14-97ce-6a39143b8e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Read the first 100 rows efficiently\n",
    "table = pq.read_table(\"yellow_tripdata_2021-01.parquet\")\n",
    "df = table.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc0b983-6bee-4e30-8320-f33a3e084379",
   "metadata": {},
   "source": [
    "##### This gets the schema from the dataframe which can be used to create a table schema in our database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36b21394-3993-45ab-94a5-548313b8f7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE \"yellow_taxi_data\" (\n",
      "\"VendorID\" INTEGER,\n",
      "  \"tpep_pickup_datetime\" TIMESTAMP,\n",
      "  \"tpep_dropoff_datetime\" TIMESTAMP,\n",
      "  \"passenger_count\" REAL,\n",
      "  \"trip_distance\" REAL,\n",
      "  \"RatecodeID\" REAL,\n",
      "  \"store_and_fwd_flag\" TEXT,\n",
      "  \"PULocationID\" INTEGER,\n",
      "  \"DOLocationID\" INTEGER,\n",
      "  \"payment_type\" INTEGER,\n",
      "  \"fare_amount\" REAL,\n",
      "  \"extra\" REAL,\n",
      "  \"mta_tax\" REAL,\n",
      "  \"tip_amount\" REAL,\n",
      "  \"tolls_amount\" REAL,\n",
      "  \"improvement_surcharge\" REAL,\n",
      "  \"total_amount\" REAL,\n",
      "  \"congestion_surcharge\" REAL,\n",
      "  \"airport_fee\" REAL\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(pd.io.sql.get_schema(df, name = 'yellow_taxi_data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e3c6b0-bcb0-4a28-a8fb-204c2acacdb4",
   "metadata": {},
   "source": [
    "#### To create a DDL statement which will create the schema in database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2deae6ed-fb2e-4ecc-ae5a-799a3d961450",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f8db528-69a8-4607-932a-db7bb9cab29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a connection to the postgres database with our credentials\n",
    "\n",
    "engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a91069-fdef-498f-b605-72e0e6f566aa",
   "metadata": {},
   "source": [
    "##### create a table with the header first.\n",
    "\n",
    "This is can be done by first sending the header alone first and then sending the data rows in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37e44bda-7295-4ca7-b6ce-1ecd1a125c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First send only table headers\n",
    "df.head(n=0).to_sql(name='yellow_taxi_data', con=engine, if_exists='replace') \n",
    "#This creates a new table in the DB and replaces any DB with same name if it exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c607156a-aaee-4469-83da-266d80d4adfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This appends the dataframe to the table if it exists (and it exists because we have created it already)\n",
    "df.to_sql(name='yellow_taxi_data', con=engine, if_exists='append')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21e655a8-7e63-430f-9d6a-c157168b9279",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted rows 0 to 100000\n",
      "Inserted rows 100000 to 200000\n",
      "Inserted rows 200000 to 300000\n",
      "Inserted rows 300000 to 400000\n",
      "Inserted rows 400000 to 500000\n",
      "Inserted rows 500000 to 600000\n",
      "Inserted rows 600000 to 700000\n",
      "Inserted rows 700000 to 800000\n",
      "Inserted rows 800000 to 900000\n",
      "Inserted rows 900000 to 1000000\n",
      "Inserted rows 1000000 to 1100000\n",
      "Inserted rows 1100000 to 1200000\n",
      "Inserted rows 1200000 to 1300000\n",
      "Inserted rows 1300000 to 1369769\n"
     ]
    }
   ],
   "source": [
    "### Running the above line of code in an iterative manner to send the data in chunks\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 100000\n",
    "\n",
    "# Open Parquet file\n",
    "parquet_file = pq.ParquetFile(\"yellow_tripdata_2021-01.parquet\")\n",
    "\n",
    "# Iterate through row groups\n",
    "for i in range(parquet_file.num_row_groups):\n",
    "    table = parquet_file.read_row_group(i)  # Read one row group at a time\n",
    "    df = table.to_pandas()  # Convert to Pandas DataFrame\n",
    "\n",
    "    # Process the DataFrame in chunks\n",
    "    for j in range(0, len(df), chunk_size):\n",
    "        df_chunk = df.iloc[j:j + chunk_size]  # Get the current chunk\n",
    "        df_chunk.to_sql(name='yellow_taxi_data', con=engine, if_exists='append', index=False)\n",
    "        print(f\"Inserted rows {j} to {j + len(df_chunk)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155c54be-9848-41ea-b492-2b94fc95d72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To read data from the SQL or write queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eada6bd2-9c5e-426b-96e7-e874af091058",
   "metadata": {},
   "source": [
    "#### To query data from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ae657016-26be-461f-be5b-f909630b0da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First create a connection to the database\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create an SQLAlchemy engine\n",
    "engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae6f09bd-ee08-4da7-b251-2ca83d744573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your query and run this:\n",
    "query = \"SELECT * FROM yellow_taxi_data LIMIT 10;\"\n",
    "\n",
    "df = pd.read_sql(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7212adce-33fe-4efd-ae7e-e7c4aac55269",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_428\\1936468720.py:13: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df2 = pd.read_sql(query, engine)\n"
     ]
    }
   ],
   "source": [
    "#Alternatively, we can connect using psycopg2\n",
    "\n",
    "# Create a psycopg2 connection\n",
    "engine = psycopg2.connect(\n",
    "    dbname=\"ny_taxi\",\n",
    "    user=\"root\",\n",
    "    password=\"root\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "\n",
    "# Use pandas to fetch data\n",
    "df2 = pd.read_sql(query, engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85fcaf1b-bffa-4739-8534-cf1bea5ba7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remember to close the connection afterwards\n",
    "engine.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec17164c-4444-4b7d-897e-918ea74a2d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Read the first 100 rows efficiently\n",
    "table = pq.read_table(\"yellow_tripdata_2021-01.parquet\")\n",
    "df = table.to_pandas()\n",
    "\n",
    "#First send only table headers\n",
    "df.head(n=0).to_sql(name='yellow_taxi_data', con=engine, if_exists='replace') \n",
    "#This creates a new table in the DB and replaces any DB with same name if it exists\n",
    "\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 100000\n",
    "\n",
    "# Open Parquet file\n",
    "parquet_file = pq.ParquetFile(\"yellow_tripdata_2021-01.parquet\")\n",
    "\n",
    "# Iterate through row groups\n",
    "for i in range(parquet_file.num_row_groups):\n",
    "    table = parquet_file.read_row_group(i)  # Read one row group at a time\n",
    "    df = table.to_pandas()  # Convert to Pandas DataFrame\n",
    "\n",
    "    # Process the DataFrame in chunks\n",
    "    for j in range(0, len(df), chunk_size):\n",
    "        df_chunk = df.iloc[j:j + chunk_size]  # Get the current chunk\n",
    "        df_chunk.to_sql(name='yellow_taxi_data', con=engine, if_exists='append', index=False)\n",
    "        print(f\"Inserted rows {j} to {j + len(df_chunk)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2611c153-6807-4bf1-ab0a-cdd06e4d33c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de9a862c-46b9-4c95-9af1-2a811a34180e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowInvalid",
     "evalue": "Unrecognized filesystem type in URI: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m parquet_file \u001b[38;5;241m=\u001b[39m pq\u001b[38;5;241m.\u001b[39mParquetFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\parquet\\core.py:310\u001b[0m, in \u001b[0;36mParquetFile.__init__\u001b[1;34m(self, source, metadata, common_metadata, read_dictionary, memory_map, buffer_size, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, filesystem, page_checksum_verification)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, source, \u001b[38;5;241m*\u001b[39m, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, common_metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    302\u001b[0m              read_dictionary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, buffer_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    303\u001b[0m              pre_buffer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, coerce_int96_timestamp_unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    304\u001b[0m              decryption_properties\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, thrift_string_size_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    305\u001b[0m              thrift_container_size_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, filesystem\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    306\u001b[0m              page_checksum_verification\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_source \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(source, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 310\u001b[0m     filesystem, source \u001b[38;5;241m=\u001b[39m _resolve_filesystem_and_path(\n\u001b[0;32m    311\u001b[0m         source, filesystem, memory_map\u001b[38;5;241m=\u001b[39mmemory_map)\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filesystem \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    313\u001b[0m         source \u001b[38;5;241m=\u001b[39m filesystem\u001b[38;5;241m.\u001b[39mopen_input_file(source)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\fs.py:179\u001b[0m, in \u001b[0;36m_resolve_filesystem_and_path\u001b[1;34m(path, filesystem, memory_map)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exists_locally:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         filesystem, path \u001b[38;5;241m=\u001b[39m FileSystem\u001b[38;5;241m.\u001b[39mfrom_uri(path)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         \u001b[38;5;66;03m# neither an URI nor a locally existing path, so assume that\u001b[39;00m\n\u001b[0;32m    182\u001b[0m         \u001b[38;5;66;03m# local path was given and propagate a nicer file not found error\u001b[39;00m\n\u001b[0;32m    183\u001b[0m         \u001b[38;5;66;03m# instead of a more confusing scheme parsing error\u001b[39;00m\n\u001b[0;32m    184\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty scheme\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \\\n\u001b[0;32m    185\u001b[0m                 \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot parse URI\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\_fs.pyx:471\u001b[0m, in \u001b[0;36mpyarrow._fs.FileSystem.from_uri\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\error.pxi:154\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\error.pxi:91\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowInvalid\u001b[0m: Unrecognized filesystem type in URI: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet"
     ]
    }
   ],
   "source": [
    "parquet_file = pq.ParquetFile(\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3a870f45-4c61-4a00-b4f7-de088b550c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import fsspec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bdd55781-18b1-4c42-8433-a38dbd94ef31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = pd.read_csv(\"taxi_zone_lookup.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e6bb39a6-794b-42b0-b555-12aa477b0bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LocationID</th>\n",
       "      <th>Borough</th>\n",
       "      <th>Zone</th>\n",
       "      <th>service_zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>EWR</td>\n",
       "      <td>Newark Airport</td>\n",
       "      <td>EWR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Queens</td>\n",
       "      <td>Jamaica Bay</td>\n",
       "      <td>Boro Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>Allerton/Pelham Gardens</td>\n",
       "      <td>Boro Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Alphabet City</td>\n",
       "      <td>Yellow Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>Arden Heights</td>\n",
       "      <td>Boro Zone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LocationID        Borough                     Zone service_zone\n",
       "0           1            EWR           Newark Airport          EWR\n",
       "1           2         Queens              Jamaica Bay    Boro Zone\n",
       "2           3          Bronx  Allerton/Pelham Gardens    Boro Zone\n",
       "3           4      Manhattan            Alphabet City  Yellow Zone\n",
       "4           5  Staten Island            Arden Heights    Boro Zone"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zones.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eebedbfd-bdf7-4db1-ae0b-27979b8ad444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zones.to_sql(name='zones', con=engine, if_exists='replace') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8978971e-4c70-4957-ad2d-e5ac67045169",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
